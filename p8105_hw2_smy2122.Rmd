---
title: "Homework 2"
author: "Sarah Younes"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options("scipen" = 1000)
```

As always, I will begin by loading the tidyverse, and I will additionally load `readxl` for Problem 2.

```{r tidyverse, message = FALSE}
library(tidyverse)
library(readxl)
```

# Problem 1

First, I will clean the data in pols-month.csv.

```{r pols_df}
pols_df =
  read.csv("./Data/fivethirtyeight_data/pols-month.csv") |>
  separate(mon, into = c("year", "month", "day"), sep = "-") |>
  mutate(
     month = replace(month, month == "01", "Jan"),
     month = replace(month, month == "02", "Feb"),
     month = replace(month, month == "03", "Mar"),
     month = replace(month, month == "04", "Apr"),
     month = replace(month, month == "05", "May"),
     month = replace(month, month == "06", "Jun"),
     month = replace(month, month == "07", "Jul"),
     month = replace(month, month == "08", "Aug"),
     month = replace(month, month == "09", "Sep"),
     month = replace(month, month == "10", "Oct"),
     month = replace(month, month == "11", "Nov"),
     month = replace(month, month == "12", "Dec")
  ) |>
  mutate(
    president = recode(prez_gop, "0" = "dem", "1" = "gop", "2" = "gop")) |>
  mutate(
    year = as.integer(year)
  ) |>
  select(year, month, everything(), -day, -starts_with("prez"))
```
    
Second, I will clean the data in snp.csv.

```{r snp_df}
snp_df =
  read.csv("./Data/fivethirtyeight_data/snp.csv") |>
  separate(date, into = c("month", "day", "year"), sep = "/") |>
  mutate(
     month = replace(month, month == "1", "Jan"),
     month = replace(month, month == "2", "Feb"),
     month = replace(month, month == "3", "Mar"),
     month = replace(month, month == "4", "Apr"),
     month = replace(month, month == "5", "May"),
     month = replace(month, month == "6", "Jun"),
     month = replace(month, month == "7", "Jul"),
     month = replace(month, month == "8", "Aug"),
     month = replace(month, month == "9", "Sep"),
     month = replace(month, month == "10", "Oct"),
     month = replace(month, month == "11", "Nov"),
     month = replace(month, month == "12", "Dec")
  ) |>
  mutate(
    year = as.integer(year)
  ) |>
  select(year, month, close)
```
   
Third, I will tidy the data in unemployment.csv.

```{r unemployment_df}
unemployment_df =
  read.csv("./Data/fivethirtyeight_data/unemployment.csv") |>
  rename(year = Year) |>
  pivot_longer(
    Jan:Dec,
    names_to = "month",
    values_to = "unemployment"
  ) |>
  mutate(month = str_to_title(month))
```

Now, I will join the three datasets into a single data frame.

```{r merge problem 1, message = FALSE}
problem_1_df =
  left_join(pols_df, snp_df) |>
  left_join(x = _, y = unemployment_df)
```

# Problem 2

First, I will import and clean the Mr. Trash Wheel dataset. Data cleaning for this dataset involves selecting the right range of data, setting missing values, cleaning the variable names, renaming two existing variables, creating two new variables, and changing the column type for one existing variable.

```{r mr_trash_wheel_df}
mr_trash_wheel_df =
  read_excel(
    "./Data/trash_wheel_data/202309 Trash Wheel Collection Data.xlsx",
    sheet = 1,
    range = "A2:M549",
    na = "") |>
  janitor::clean_names() |>
  rename(
    weight = weight_tons,
    volume = volume_cubic_yards) |>
  mutate(
    homes_powered = (weight*500)/30
  ) |>
  mutate(
    trash_wheel = "mr trash wheel"
  ) |>
  mutate(
    year = as.integer(year)
  )
```

Second, I will import and clean the Professor Trash Wheel dataset. Data cleaning for this dataset involves selecting the right range of data, setting missing values, cleaning the variable names, renaming two existing variables, and creating two new variables.

```{r prof_trash_wheel_df}
prof_trash_wheel_df =
  read_excel(
    "./Data/trash_wheel_data/202309 Trash Wheel Collection Data.xlsx",
    sheet = 2,
    range = "A2:L96",
    na = "") |>
  janitor::clean_names() |>
  rename(
    weight = weight_tons,
    volume = volume_cubic_yards) |>
  mutate(
    homes_powered = (weight*500)/30
  ) |>
  mutate(
    trash_wheel = "professor trash wheel"
  )
```

Third, I will import and clean the Professor Trash Wheel dataset. Data cleaning for this dataset involves selecting the right range of data, setting missing values, cleaning the variable names, renaming two existing variables, and creating two new variables.

```{r gwynnda_trash_wheel_df}
gwynnda_trash_wheel_df =
  read_excel(
    "./Data/trash_wheel_data/202309 Trash Wheel Collection Data.xlsx",
    sheet = 4,
    range = "A2:J108",
    na = "") |>
  janitor::clean_names() |>
  rename(
    weight = weight_tons,
    volume = volume_cubic_yards) |>
  mutate(
    homes_powered = (weight*500)/30
  ) |>
  mutate(
    trash_wheel = "gwynnda trash wheel"
  )
```

Finally, I will merge all three datasets by using `bind_rows` instead of another join or merge type because most of the column names are consistent across datasets and rows just need to be added to create one horizontally-longer dataset.

```{r merge problem 2}
merged_trash_wheel_df =
  bind_rows(mr_trash_wheel_df, prof_trash_wheel_df, gwynnda_trash_wheel_df) |>
  select(trash_wheel, everything())
```

Each dataset describes trash collected by different trash wheels around Baltimore, Maryland from May 2014 to July 2022. The first dataset in this problem, Mr. Trash Wheel, contains `r nrow(mr_trash_wheel_df)`  observations and `r ncol(mr_trash_wheel_df)` variables. The second dataset in this problem, Professor Trash Wheel, contains `r nrow(prof_trash_wheel_df)` observations and `r ncol(prof_trash_wheel_df)` variables. The third dataset in this problem, Gwynnda Trash Wheel, contains `r nrow(gwynnda_trash_wheel_df)` observations and `r ncol(gwynnda_trash_wheel_df)` variables. Each row represents one different dumpster that the trash wheel picks up trash from; thus, Mr. Trash Wheel collects trash from the most dumpsters, followed by Gwynnda Trash Wheel and Professor Trash Wheel. The columns for each dataset tell us the `date` of trash collection, `weight` (in tons) and `volume` (in cubic yards) of that dumpster's trash, and different types of trash that the trash wheels have collected, such as `plastic bottles` and `grocery bags`. The merged dataset with all three trash wheels contains `r nrow(merged_trash_wheel_df)` observations and `r ncol(merged_trash_wheel_df)` variables.  Altogether, the three trash wheels collected `r sum(pull(merged_trash_wheel_df, weight)) |> round(2)` tons of trash and `r sum(pull(merged_trash_wheel_df, volume)) |> round(2)` cubic pounds of trash. Professor Trash Wheel alone collected `r filter(merged_trash_wheel_df, trash_wheel == "professor trash wheel") |> pull(weight) |> sum() |> round(2)` tons of trash. Altogether, the three trash wheels collected `r filter(merged_trash_wheel_df, month == "July", year == 2021) |> pull(cigarette_butts) |> sum() |> round(2)` cigarette butts in July of 2021, while Gwynnda Trash Wheel alone collected `r filter(merged_trash_wheel_df, month == "July", year == 2021, trash_wheel == "gwynnda trash wheel") |> pull(cigarette_butts) |> sum() |> round(2)` cigarette butts in July of 2021. Additionally, the three trash wheels collectively powered `r sum(pull(merged_trash_wheel_df, homes_powered)) |> round(2)` homes.

# Problem 3

First, I will import and clean the dataset of baseline demographics. Data cleaning for this dataset first involves skipping the first row of non-data, setting missing values, cleaning the variable names, renaming an existing variable, and recoding two existing variables, and removing participants who do not meet the inclusion criteria.

```{r demographics_df}
demographics_df =
  read.csv(
    "./Data/data_mci/MCI_baseline.csv",
    skip = 1,
    na = "."
  ) |>
  janitor::clean_names() |>
  rename(
    baseline_age = current_age) |>
  mutate(
    sex =
      case_match(
        sex,
        0 ~ "male",
        1 ~ "female"),
        sex = as.factor(sex)
  ) |>
  mutate(
    apoe4 =
      case_match(
        apoe4,
        0 ~ "non carrier",
        1 ~ "carrier"),
        apoe4 = as.factor(apoe4)
  ) |>
  filter(baseline_age < age_at_onset | is.na(age_at_onset))
```

The above dataset includes basic demographic information of potential and actual participants for the observational study. It includes only 6 variables collected at baseline: participants' study ID, current age, sex, years of education, whether they carry the APOE4 variant, and age at onset of Mild Cognitive Impairment (MCI). Arguably the most important use of this dataset is its ability to screen whether potential participants qualify of the study, since one of the major inclusion criteria is that participants must be free of MCI at baseline. Therefore, one of the most important steps in the import process was setting potential participants who did not have an age at onset for MCI as missing or NA and filtering out participants who had an age at onset for MCI that predated the baseline study.

After this process, a total of `r nrow(demographics_df)` participants were recruited or qualified for the observational study. Among these, `r filter(demographics_df, age_at_onset > baseline_age) |> count()` participants developed MCI during the study. The average baseline age of participants who qualified was `r round(mean(pull(demographics_df, baseline_age)), 2)` years old. Additionally, `r round((sum(demographics_df$sex == "female" & demographics_df$apoe4 == "carrier"))/sum(demographics_df$sex == "female")*100, 2)`% of women who qualified for the study were carriers of the APOE4 gene.

Second, I will import and clean the dataset of longitudinally observed biomarker values. Data cleaning for this dataset involves skipping the first row of non-data, setting missing values, cleaning the variable names, tidying the data into a longer table, and clarifying category labels, and renaming an existing variable.

```{r amyloid_df}
amyloid_df =
  read.csv(
    "./Data/data_mci/mci_amyloid.csv",
    skip = 1,
    na = c("NA", "Na")
  ) |>
  janitor::clean_names() |>
  pivot_longer(
    baseline:time_8,
    names_to = "year",
    values_to = "biomarker_ratio"
  ) |>
  mutate(
    year = replace(year, year == "baseline", "0"),
    year = replace(year, year == "time_2", "2"),
    year = replace(year, year == "time_4", "4"),
    year = replace(year, year == "time_6", "6"),
    year = replace(year, year == "time_8", "8")
  ) |>
  rename(id = study_id)
```

The amyloid dataset now consists of 3 columns instead of 6: one column for the `study ID`, another for the `time period (in years)` that is being measured, and a final column for the `biomarker ratio value` at each time period.

Now, I will check if some participants appear in only the baseline or amyloid datasets by doing an anti-join.

```{r anti-joins}
anti_join(demographics_df, amyloid_df, by = "id") |>
  distinct(id) |> 
  nrow()

anti_join(amyloid_df, demographics_df, by = "id") |>
  distinct(id) |> 
  nrow()
```

The output of the first anti-join showed that 8 participants were in the baseline demographics dataset but not the amyloid dataset. The output of the second anti-join showed that 16 different participants were int the amyloid dataset but not the baseline demographics dataset.

Now, I will combine the baseline and amyloid datasets so that only participants who appear in both datasets are retained by using an inner join.

```{r merge problem 3}
merged_alzheimers_df =
  inner_join(amyloid_df, demographics_df, by = "id")
```

The merged dataset contains `r nrow(merged_alzheimers_df)` observations and `r ncol(merged_alzheimers_df)` variables. There are `r merged_alzheimers_df |> distinct(id) |>nrow()` total participants in this dataset, with multiple rows or observations per participant to reflect the different follow-up times over the 8-year study period. The average biomarker ratio value was `r round(mean(merged_alzheimers_df$biomarker_ratio, na.rm = TRUE), 2)`.

Finally, I will export the resulting merged dataset as a CSV to my data directory.

```{r export}
write_csv(merged_alzheimers_df, "alzheimers_dataset.csv")
```